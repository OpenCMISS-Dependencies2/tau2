{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tau_profile_parser import TauProfileParser\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions: \n",
    "Drag this notebook into the main folder that contains the TauProfile parent folders.  \n",
    "  \n",
    "Example:  \n",
    "/documents/experimentName/folder1/profile.0.0.0   \n",
    "/documents/experimentName/folder1/profile.1.0.0  \n",
    "/documents/experimentName/ThisNotebook  \n",
    "  \n",
    "  \n",
    "#### UI  \n",
    "At minimum, the user need only change a few key variables listed throughout the notebook.  \n",
    "The notebook will read the files from the local folder.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_bool = False # if working with mutliple timers, e.g. TIME, Papi_L1_DCM, etc.\n",
    "counter_data = True\n",
    "cores_per_node = 42\n",
    "# print(os.getcwd()) # Verify working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ExampleData = os.getcwd()\n",
    "directory_contents = os.listdir(path_to_ExampleData)\n",
    "folders = [item for item in directory_contents if os.path.isdir(item) and item[0] != '.']\n",
    "\n",
    "# sort by number of ranks\n",
    "# print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Logic\n",
    "# if all false, then no multi files, but if one is True then there is a MULTI file\n",
    "\n",
    "multi_bool = False\n",
    "for fold in folders:\n",
    "    subdirs_names = os.listdir(path_to_ExampleData+'/'+fold)\n",
    "    subfolders = [item for item in directory_contents if os.path.isdir(item) and item[0] != '.']\n",
    "    if any([False if 'MULTI' not in fold else True for subfold in subfolders]):\n",
    "        multi_bool = True\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TauProfObjects = [TauProfileParser.parse(path_to_ExampleData+'/'+fold, MULTI=multi_bool) for fold in folders]\n",
    "\n",
    "# orig = TauProfileParser.parse(path_to_ExampleData+'/orig',MULTI=multi_bool)\n",
    "# nocall = TauProfileParser.parse(path_to_ExampleData+'/nocall',MULTI=multi_bool)\n",
    "\n",
    "print(TauProfObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see details of the run, metadata will print out information\n",
    "\n",
    "# TauProfObjects[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframes from a tau profile object\n",
    "\n",
    "Using the Atomic data from counters. The TauProfileParser obeject uses .atomic_data() to return a dataframe of the counters.\n",
    "For timer data, .interval_data() is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter_data:\n",
    "    Dataframes1 = [obj.atomic_data()   for obj in TauProfObjects]\n",
    "else:\n",
    "    Dataframes1 = [obj.interval_data() for obj in TauProfObjects]\n",
    "\n",
    "print(type(Dataframes1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of nodes for each experiment\n",
    "Dataframes = [(int(len(set(frame.index.get_level_values('Node'))))/cores_per_node,frame) for frame in Dataframes1 ]\n",
    "Dataframes = sorted(Dataframes, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in Dataframes:\n",
    "    pair[1]['Total'] = pair[1]['Count'] * pair[1]['Mean']\n",
    "    \n",
    "papi_timers = [timer for timer in set(Dataframes[0][1].index.get_level_values(\"Timer\")) if 'PAPI_' in timer]\n",
    "papi_timers.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataframes[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = \"Maximum\"  # User can change to one of the column names\n",
    "\n",
    "def get_non_correlated_timers(dataframe, papi_timers, metric=Metric, corr_threshold=0.9):\n",
    "    # Returns a dataframe of only correlated timers using pandas.corr()\n",
    "    # organized with rows of nodes and columns being the timers.\n",
    "    # corr_report\n",
    "    dataframe_of_metric = pd.DataFrame()\n",
    "    for node in set(dataframe.index.get_level_values('Node')):\n",
    "        values = dataframe.loc[(node,0,0,papi_timers),(metric)].T\n",
    "        values.reset_index(level=['Node','Context','Thread'], drop=True, inplace=True)\n",
    "        dataframe_of_metric = dataframe_of_metric.append(values)\n",
    "        \n",
    "    dataframe_of_metric.reset_index(drop=True, inplace=True)\n",
    "    correlation_frame = dataframe_of_metric.corr()\n",
    "    rows = list(correlation_frame.index)\n",
    "    interesting_pairs = papi_timers.copy()\n",
    "    corr_report = {}\n",
    "    for col in correlation_frame.columns:\n",
    "        for row in rows:\n",
    "            if correlation_frame.loc[(row),(col)] >= corr_threshold and row != col and row in interesting_pairs and col in interesting_pairs:\n",
    "                #interesting_pairs.remove(random.choice([row,col]))\n",
    "                interesting_pairs.remove(col)\n",
    "                corr_report[(row,col)] = correlation_frame.loc[(row),(col)]\n",
    "        rows.remove(col)\n",
    "    interesting_pairs.remove('PAPI_SP_OPS')\n",
    "    interesting_pairs.remove('PAPI_HW_INT')\n",
    "\n",
    "    return dataframe_of_metric, set(interesting_pairs), pd.Series(corr_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation threshold percentage\n",
    "threshold = 0.9\n",
    "\n",
    "# each entry in corr_dfs: ('folder name', ('metric df', 'non-correlated timers', 'dropped timers with correlation numbers'))\n",
    "corr_dfs = [(pair[0], get_non_correlated_timers(pair[1], papi_timers)) for pair in Dataframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dfs[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(df1, noutliers=5):\n",
    "    # Takes a dataframe and returns a smaller dataframe using the pandas sample().\n",
    "    # The commented section adds in n max & min outliers for each column.\n",
    "    \n",
    "    df = df1.copy()\n",
    "    new_df = pd.DataFrame()\n",
    "#     for _ in range(5): # add mins\n",
    "#         maxes = df.idxmax()\n",
    "#         for m in maxes:\n",
    "#             new_df = new_df.append(df.loc[m].T)\n",
    "#             df.drop([m])\n",
    "#         mins = df.idxmin()\n",
    "#         for n in mins:\n",
    "#             new_df = new_df.append(df.loc[n].T)\n",
    "#             df.drop([n])\n",
    "    new_df = new_df.append(df.sample(frac=.25, random_state=1))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dfs = [(tup[0], samples(tup[1][0]), tup[1][1], tup[1][2]) for tup in corr_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the metric dataframes with columns of numbered index and method names to distinguish in graphs\n",
    "\n",
    "for tup in corr_dfs:\n",
    "    tup[1][0]['Node'] = list(tup[1][0].index)\n",
    "    tup[1][0]['Method'] = [str(tup[0]) + ' nodes' for i in range(len(tup[1][0].index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in sampled_dfs:\n",
    "    tup[1]['Node'] = list(tup[1].index)\n",
    "    tup[1]['Method'] = [str(tup[0]) + ' nodes' for i in range(len(tup[1].index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [tup[1][1] for tup in corr_dfs]\n",
    "intersection_timers = list(set.intersection(*sets)).sort() # whats common between the runs\n",
    "union_timers = list(set.union(*sets)).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_splom = pd.concat([tup[1][0] for tup in corr_dfs]) # non-sampled, very large dataset\n",
    "sampled_comb_df = pd.concat([tup[1] for tup in sampled_dfs]) # sampled, much smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfig = px.histogram(combined_df_splom, x=\"PAPI_STL_ICY\", color=\"Method\", marginal=\"box\")\n",
    "hfig.update_layout(height=600, width=800, title=\"Unsampled PAPI_STL_ICY\")\n",
    "hfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfig = px.histogram(sampled_comb_df, x=\"PAPI_STL_ICY\", color=\"Method\", marginal=\"box\")\n",
    "hfig.update_layout(height=600, width=800, title=\"Sampled PAPI_STL_ICY\")\n",
    "hfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the case with lots of data points, saving the helps improve visual performance.\n",
    "dimensions = union_timers # can change to intersection timer set\n",
    "\n",
    "\n",
    "title='Timers with correlated removed (>{:.2f}) and sampled data'.format(threshold)\n",
    "fig2 = px.scatter_matrix(sampled_comb_df, dimensions=dimensions, color='Method', hover_data=['Node'])\n",
    "fig2.update_layout(height=3000, width=3000, title=title)\n",
    "#fig2.write_html(path_to_ExampleData+'/1Combined_64v63_SPLOM.html') # uncomment to save to file for easier viewing\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
